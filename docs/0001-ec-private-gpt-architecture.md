# Project's Architecture

## Context

The ec-private-gpt project follows the [PrivateGPT](https://docs.privategpt.dev/) project architecture to deploy an in-house RAG (Retrieval-Augmented Generation) model.

The project is designed to be a standalone service that can generate text based on context and a prompt. It is designed to be used in a microservice architecture, deployed as a service that other services can access through an API.

## Decision

The project is divided into the following components:

- **API**: The API is the entry point to the service. It is responsible for receiving requests, validating them, and forwarding them to the model for processing. It is also responsible for returning the results to the client.
- **Embeddings Model**: The Embeddings Model generates embeddings for the input context (docs, spreadsheets, etc.)
- **Inference Model**: The Inference Model generates text based on the input context and prompt. It uses the embeddings generated by the Embeddings Model to create the text.
- **Vectors Database**: The Vectors Database stores the embeddings generated by the LLM Embeddings Model. The Inference Model uses it to retrieve the embeddings for the input context.
- **Chat Frontend**: The Chat Frontend is a web interface that allows users to interact with the model.

### API

The API will be hosted and served on AWS. It will be containerized using Docker and deployed using ECS (Elastic Container Service).

Its code will be versioned and stored in a private repository on GitHub and built and deployed to AWS using GitHub Actions.

![API Architecture](ec-private-gpt-api-architecture.png)

### LLM Embeddings and Inference Models

At the current stage, we will use OpenAI's GPT models for generating embeddings and inference. The models are hosted and served by OpenAI, and we will communicate with them using their API.

In the future, we may train our models and host them ourselves. In that case, we will use the same architecture as the API to host and serve the models.

### Vectors Database

We decided to use [Qdrant](https://qdrant.tech/) as our vectors database.

Qdrant is an open-source vector database that stores and retrieves high-dimensional vectors efficiently. It is designed to be used with large-scale machine learning models and optimized for fast retrieval of vectors.

At the current stage of development, we will use [Qdrant's cloud service](https://cloud.qdrant.io/) to store and retrieve the embeddings. In the future, we may decide to host our instance of Qdrant.

### Chat Frontend

TBD.

## Consequences

The architecture is designed to be scalable and maintainable. It is designed to be used in a microservice architecture, where the model is deployed as a service and can be accessed by other services through an API.

### Makes easier

- The architecture makes it easier to deploy and scale the model. Having a standalone service that is fully CI/CD integrated allows us to deploy new versions of the model quickly and easily.
- Using managed services (OpenAI models and Qdrant cloud service) allows us to focus on developing the API and the Chat Frontend without worrying about the model infrastructure and the vector database.
- The architecture allows us to use the same codebase and deployment process for the LLM Embeddings and Inference Models if we decide to host them ourselves.

### Makes harder

- Models customization: using managed services means that we are limited to the features and capabilities provided by the services. If we decide to host the models ourselves in the future, we will have to develop and maintain the infrastructure for the models.
- Privacy and PII: using managed services means that we'll be sending user data to third-party services. We must comply with privacy regulations and not send sensitive data to the services.
