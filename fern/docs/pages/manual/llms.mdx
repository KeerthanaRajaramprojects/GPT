## Running the Server

PrivateGPT supports running with different LLMs & setups.

### Local models

Both the LLM and the Embeddings model will run locally.

Make sure you have followed the *Local LLM requirements* section before moving on.

This command will start PrivateGPT using the `settings.yaml` (default profile) together with the `settings-local.yaml`
configuration files. By default, it will enable both the API and the Gradio UI. Run:

```bash
PGPT_PROFILES=local make run
```

or

```bash
PGPT_PROFILES=local poetry run python -m private_gpt
```

When the server is started it will print a log *Application startup complete*.
Navigate to http://localhost:8001 to use the Gradio UI or to http://localhost:8001/docs (API section) to try the API
using Swagger UI.

### Using OpenAI

If you cannot run a local model (because you don't have a GPU, for example) or for testing purposes, you may
decide to run PrivateGPT using OpenAI as the LLM and Embeddings model.

In order to do so, create a profile `settings-openai.yaml` with the following contents:

```yaml
llm:
  mode: openai

openai:
  api_key: <your_openai_api_key>  # You could skip this configuration and use the OPENAI_API_KEY env var instead
  model: <openai_model_to_use> # Optional model to use. Default is "gpt-3.5-turbo"
                               # Note: Open AI Models are listed here: https://platform.openai.com/docs/models
```

And run PrivateGPT loading that profile you just created:

`PGPT_PROFILES=openai make run`

or

`PGPT_PROFILES=openai poetry run python -m private_gpt`

When the server is started it will print a log *Application startup complete*.
Navigate to http://localhost:8001 to use the Gradio UI or to http://localhost:8001/docs (API section) to try the API.
You'll notice the speed and quality of response is higher, given you are using OpenAI's servers for the heavy
computations.

### Using AWS Sagemaker

For a fully private & performant setup, you can choose to have both your LLM and Embeddings model deployed using Sagemaker.

Note: how to deploy models on Sagemaker is out of the scope of this documentation.

In order to do so, create a profile `settings-sagemaker.yaml` with the following contents (remember to
update the values of the llm_endpoint_name and embedding_endpoint_name to yours):

```yaml
llm:
  mode: sagemaker

sagemaker:
  llm_endpoint_name: huggingface-pytorch-tgi-inference-2023-09-25-19-53-32-140
  embedding_endpoint_name: huggingface-pytorch-inference-2023-11-03-07-41-36-479
```

And run PrivateGPT loading that profile you just created:

`PGPT_PROFILES=sagemaker make run`

or

`PGPT_PROFILES=sagemaker poetry run python -m private_gpt`

When the server is started it will print a log *Application startup complete*.
Navigate to http://localhost:8001 to use the Gradio UI or to http://localhost:8001/docs (API section) to try the API.



### Use AWS Bedrock Models

Another alternative if you don't have the necessary GPU to run local models, is using AWS Bedrock models. The default chosen models in Bedrock are Anthropic Claude v2 (LLM) and Titan Embeddings G1 - Text (embedding). You can navigate to `settings-bedrock.yaml` and change the `llm_modelid` and `embedding_modelid` parameters. You can find the available models and their IDs in the link https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html

In order to use the Bedrock models, you need to do the following configurations.

#### Install AWS CLI

* Go to `https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html` and follow the instructions

#### Configure AWS CLI

* Open a powershell and run the following command: `aws configure`

* You'll be prompted to enter the following information:


  * AWS Access Key ID\
  AWS Secret Access Key\
  Default region name\
  Default output format (optional)\
  These credentials should have the necessary permissions to interact with AWS services.

#### Set the Environment Variable

To add Bedrock as an active profile of PrivateGPT, you should add `'bedrock'` as value to an environment variable named **PGPT_PROFILES**:

First, check if the variable exists with one of the commands below based on your OS. If the variable exists, it will print the value. If not, it will do nothing:

```bash
# macOS (terminal)
echo $PGPT_PROFILES

# Windows (cmd)
echo %PGPT_PROFILES%

# Windows (Powershell)
$env:PGPT_PROFILES

# Linux (Bash)
echo $PGPT_PROFILES
```

If variable does not exist, set the value to `'bedrock'` with one the following commands:

```bash
# macOS (terminal)
export PGPT_PROFILES="bedrock"

# Windows (cmd)
set PGPT_PROFILES=bedrock

# Windows (Powershell)
$env:PGPT_PROFILES="bedrock"

# Linux (Bash)
export PGPT_PROFILES="bedrock"
```

If variable does exist, add the value `'bedrock'` to env var with one the following commands:

```bash
# macOS (terminal)
export PGPT_PROFILES="${PGPT_PROFILES}:bedrock"

# Windows (cmd) : Windows Command Prompt doesn't have a straightforward way to directly append to an environment variable without extra tooling or scripting.

# Windows (Powershell)
$env:PGPT_PROFILES += ";bedrock"

# Linux (Bash)
export PGPT_PROFILES="${PGPT_PROFILES}:bedrock"
```

#### Run the program

Then, you can run the program with the command:

```bash
poetry run python -m private_gpt
```